{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10p7ASBiq9AJ"
   },
   "source": [
    "# Visão Computacional em Python utilizando as bibliotecas Scikit-Image e Scikit-Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-D9VvLaq9AL"
   },
   "source": [
    "### Algoritmos de visão computacional tem como entrada matrizes bidimensionais ou multidimensionais e produzem uma informação simples como saída.\n",
    "\n",
    "**Etapas do PDI**\n",
    "\n",
    "* Segmentação\n",
    "* Extração\n",
    "* Classificação\n",
    "\n",
    "**Áreas**\n",
    "\n",
    "* Visão Computacional: entra imagem, sai dado.\n",
    "* Computação Gráfica: entra dado, sai imagem.\n",
    "* PDI: Entra imagem, sai imagem.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxp975oaq9AN"
   },
   "source": [
    "# Segmentação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eXS4DoCq9AO"
   },
   "source": [
    "* Agrupamento de pixels em uma componente conexa relevante.\n",
    "* Subdivisões da imagem em regiões ou objetos segundo um critério.\n",
    "* Divide uma região espacial R em N subregiões.\n",
    "* Limiarização \n",
    "* Etapa Fundamental: Implementação de algoritmos para o reconhecimento das diferentes regiões da imagem através das características dos pixels ou distribuição deles na imagem.\n",
    "* Calculo do valor ótimo de limiar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0I6d1AIq9AT"
   },
   "source": [
    "## Código para segmentação em python usando Scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZ9peHoWq9AW"
   },
   "source": [
    "### Importando Bibliotecas e Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUyz20SSq9AY"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave #funcao do pacote skimage para leitura de imagens, recebe o endereço da imagem como parâmetro\n",
    "from skimage.color import rgb2grey #transforma uma magem RGB(colorida) em níveis de cinza, recebe imagem como parâmetro\n",
    "from skimage.filters import threshold_otsu #funcao que implementa o limiar de Otsu, imagem em tons de cinza como parâmetro\n",
    "import numpy as np #pacote python que oferece suporte a arrays e matrizes multidimensionais\n",
    "from skimage.measure import label, regionprops #label: Detecta pequenas regiões que não fazem parte da região de interesse\n",
    "                                               #regionprops: cálculo de propriedades importantes em cada região encontrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGsxm-iaq9Ad"
   },
   "source": [
    "### Carregando imagem, convertendo para níveis de cinza e calculando o limiar usando otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3D6hUg8q9Af"
   },
   "outputs": [],
   "source": [
    "path_image = \"databases-master/database2/imagem (1).png\" #variavel recebe o endereço da imagem\n",
    "imagem = imread(path_image) #Leitura da imagem\n",
    "grey_image = rgb2grey(imagem) #Convertendo a imagem de colorido para tons de cinza\n",
    "otsu = threshold_otsu(grey_image) #Realizando o calculo do limiar usando otsu\n",
    "img_otsu = grey_image < otsu #Pixels com valores menores que 'otsu' serão brancos, caso contrario serao pretos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7m3KDMGSq9Al"
   },
   "source": [
    "### Remoção de pequenas regiões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZd_yk-3q9An"
   },
   "outputs": [],
   "source": [
    "#Detecção e cálculo de propriedades\n",
    "label_img = label(img_otsu, connectivity = grey_image.ndim) #Detecta regiões não conectadas\n",
    "props = regionprops(label_img) #Calcula propriedades importantes de cada região encontrada (ex.: area)\n",
    "\n",
    "#Converte todas as regiões que possuem um valor de área menor que a maior area em background da imagem\n",
    "area = np.asarray([props[i].area for i in range(len(props))]) #Área de cada região encontrada\n",
    "max_index = np.argmax(area) #Index da maior região\n",
    "\n",
    "for i in range(len(props)):\n",
    "    if (props[i].area < props[max_index].area):\n",
    "        label_img[np.where(label_img == i+1)] = 0 #Região menor que a maior é marcada como background        \n",
    "\n",
    "label_img = label_img*1 #Transforma imagem de valores booleanos para inteiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EicTxLqHq9Aw"
   },
   "source": [
    "### Recorte da região de interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ch06FwMtq9Ay"
   },
   "outputs": [],
   "source": [
    "#Obtendo os limites verticais das imagens segmentadas\n",
    "ymin = np.min(np.where(label_img == True)[1])\n",
    "ymax = np.max(np.where(label_img == True)[1])\n",
    "imagem_segmentada = imagem[:,ymin:ymax,:]\n",
    "imsave(\"imagemseg.png\", imagem_segmentada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPQ_4o7Oq9A3"
   },
   "source": [
    "***\n",
    "# Extração de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9O7AY07Kq9A4"
   },
   "source": [
    "* As características (propriedades) dos objetos na imagem podem ser quantificadas e usadas como descritores da imagem ou de objetos em uma cena.\n",
    "* O objeto pode ser representado por um espaço R^n, definido em termos das N características\n",
    "* Invariância a transformações afins: invariante a rotação, escala e translação (Propriedade desejável a descritores).\n",
    "* Modos de representar regiões (Passo essencial):\n",
    "    * Através de características externas, pertencentes a fronteira.\n",
    "    * Em termos de características internas, pixels que compõem a região.\n",
    "* Descrever a região com base na representação escolhida.\n",
    "* Informações adquiridas na extração de atributos formam um vetor de atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgS9Dxunq9A6"
   },
   "source": [
    "## Matriz de Coocorrência em Níveis de Cinza (GLCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-R-lB9Vq9A_"
   },
   "source": [
    "* Técnica que tem como base a análise de textura em imagem.\n",
    "* Análise de coocorrências existentes entre pares de pixels através de algum padrão.\n",
    "* Sempre quadrada, guarda informações das intensidades relativas dos pixels.\n",
    "* As coocorrências são calculadas para determinadas distâncias ou espaçamento entre pares de pixels e uma orientação.\n",
    "* Existe uma matriz para cada relacionamento entre distância e orientação.\n",
    "* De acordo com Haralick, são 14 características significativas para GLCM e outras medidas derivadas foram acrescentadas.\n",
    "    * A utilização de apenas um subconjunto dessas características pode (em alguns casos) gerar melhor desempenho do que a utilização de todas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW_qL6dUq9BA"
   },
   "source": [
    "## Código para a extração de atributos em Python usando Scikit-Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOjD0O19q9BC"
   },
   "source": [
    "* Realizada em duas etapas:\n",
    "    * Cálculo da matriz de coocorrências \n",
    "    * Cálculo dos atributos de Haralick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNcXbEvhq9BD"
   },
   "source": [
    "### Importando Bibliotecas e Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfMw_4fNq9BF"
   },
   "outputs": [],
   "source": [
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "#greycomatrix: realiza o cálculo da matriz. Parâmetros: imagem, distância, ângulo, normed (se true, retorna matriz normalizada)\n",
    "#greycoprops: calcula os atributos de Haralick. Parâmetros: matriz calculada e o atributo a ser calculado.\n",
    "#image_as_ubyte: conversão de valores na matriz da imagem. Converte para valores unsigned integer entra 0 e 255. Parâmetros: matriz da imagem a ser convertida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-7jFKFKq9BM"
   },
   "source": [
    "### Cálculo da matriz GLCM e dos Atributos Haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0TmEMfcq9BO",
    "outputId": "ec0eab33-47db-423f-adf1-177a004069f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acucena/anaconda3/lib/python3.7/site-packages/skimage/util/dtype.py:130: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "image_path = \"imagemseg.png\" #Endereço da imagem\n",
    "image = imread(image_path) #Leitura da imagem\n",
    "image_grey = rgb2grey(imagem) #Convertendo de RGB para tons de cinza\n",
    "img = img_as_ubyte(image_grey)\n",
    "d = 1 #Distância entre pixels GLCM\n",
    "matrix = greycomatrix(img, [d], [0], normed = True) #Cálculo da matriz em 0 graus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIlN6Xykq9BV"
   },
   "outputs": [],
   "source": [
    "props = np.zeros((6)) #Vetor para armazenar atributos\n",
    "props[0] = greycoprops(matrix, 'contrast') #Calcula contrast \n",
    "props[1] = greycoprops(matrix, 'dissimilarity') #Calcula dissimilarity \n",
    "props[2] = greycoprops(matrix, 'homogeneity') #Calcula homogeneity \n",
    "props[3] = greycoprops(matrix, 'energy') #Calcula energy \n",
    "props[4] = greycoprops(matrix, 'correlation') #Calcula correlation \n",
    "props[5] = greycoprops(matrix, 'ASM') #Calcula segundo momento angular "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Aprendizagem de Máquina e Classificação de Imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reconhecimento de padrões: classificação de determinado objeto ou situação como pertencente a um certo número de categorias.\n",
    "* Algoritmos de aprendizado de máquina supervisionados: algoritmos em que é preciso apresentar dados pré-classificados e \"ensinar\" o algoritmo a identificar diferentes objetos.\n",
    "    * Necessário que ao menos parte dos dados sejam previamente avaliados e rotulados por especialistas.\n",
    "    * Ex.: SVM, MLP, RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Máquina de Vetor de Suporte - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Faz um mapeamento do espaço de entrada para um espaço de dimensionalidade maior.\n",
    "* Calcula um hiperplano de separação ótimo.\n",
    "    * Escolhido de modo a maximizar a distância de separação entre as classes.\n",
    "    * Duas classes são linearmente separáveis se existe um hiperplano divisório entre as amostras de classes diferentes.\n",
    "    * O que é um hiperplano?\n",
    "        * Trata-se de uma linha que ao ser traçada consegue separar duas classes distintas em cada parte do plano cartesiano.\n",
    "    * O que é o hiperplano de separação ideal?\n",
    "        * É um hiperplano que é tão longe quanto possível dos pontos de cada categoria de forma que ele classifica os dados de maneira correta e generaliza melhor com dados não usados no treinamento.\n",
    "* Trata-se de um algoritmo supervisionado: possui fases de treinamento e teste.\n",
    "\n",
    "* Vetores de suporte\n",
    "    * Projetados no treinamento.\n",
    "    * Usados para encontrar um hiperplano de separação ótimo.\n",
    "* Como funciona\n",
    "    * Aumenta a dimensionalidade da entrada para transformar funções não-linearmente separáveis em funções linearmente separáveis.\n",
    "    * Funções de Kernel:\n",
    "        * Utilizadas para aumentar a dimensão de um vetor de entrada.\n",
    "        * Aumento de uma dimensão N para uma X, sendo X > N.\n",
    "    * Após a realização dos cálculos, são obtidos os vetores de suporte e a partir deles é obtido o hiperplano de separação ótimo, onde cada lado do mesmo representa uma classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combinação de predições de diversas árvores em que cada árvore depende dos valores de um vetor independente, amostrados aleatoriamente e com a mesma distribuição para cada árvore da floresta.\n",
    "    * Floresta: coleção de árvores de decisão.\n",
    "* Flexível e fácil de usar(Dois parâmetros: nº de variáveis do subconjunto aleatório em cada nó e o número de árvores da floresta). Produz excelentes resultados, na maioria das vezes.\n",
    "* Como funciona?\n",
    "    * Algoritmo de aprendizagem supervisionado.\n",
    "    * Cria uma floresta de modo aleatório.\n",
    "        * Combinação de árvores de decisão, em sua maioria, treinadas com o método bagging.\n",
    "        * Método bagging: combinação dos modelos de aprendizado aumenta o resultado geral.\n",
    "    * Resumindo: cria várias árvores de decisão e as combina para obter uma predição com maior acurácia e mais estável.\n",
    "    * Adiciona aleatoriedade extra ao modelo quando está criando as árvores\n",
    "        * Procuram a melhor característica ao dividir os nós.\n",
    "        * As características se subdividem aleatoriamente em subconjuntos(vetores) de características.\n",
    "        * Busca a melhor característica dentro dos subconjuntos. Para cada árvore é gerado um subconjunto de características.\n",
    "    * Escolhe as classes com o maior número de votos na floresta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para classificação de imagens usando SVM e RF em Python usando Scikit-Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-learn: Funções de aprendizagem de máquina, clusterização, seleção de atributos e outras.\n",
    "* Etapas básicas da classificação:\n",
    "    * Divisão do conjunto de dados em treino e teste.\n",
    "    * Criação de uma instância do classificador utilizado.\n",
    "    * Treino do classificador.\n",
    "    * Predição utilizando casos de teste.\n",
    "    * Cálculo da taxa de acertos seguindo alguma métrica de avaliação de desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Bibliotecas e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC #SVM para classificação\n",
    "from sklearn.ensemble import RandomForestClassifier #RF para classificação\n",
    "from sklearn.model_selection import train_test_split #Pacote para dividir os dados em treino e teste\n",
    "from sklearn.metrics import accuracy_score #Métrica de Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do conjunto de dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Não executar, não funciona pois é um exemplo de como deve ser aplicada a função e não a aplicação em si.\n",
    "train = 0.3 #30% dos dados para treino\n",
    "test = 1-train #Resto dos dados(70%) para testar o classificador \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=test) #Divisão dos dados em conjuntos de treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instância, treino, teste e cálculo de acertos usando SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não executar, não funciona pois é um exemplo de como deve ser aplicada a função e não a aplicação em si.\n",
    "cfl_svm = SVC() #Instância da SVM\n",
    "clf_svm.fit(X_train, Y_train) #Treinamento da SVM\n",
    "pred_svm = clf_svm(X_test) #Teste da SVM\n",
    "acc_svm = accuracy_score(Y_test, pred_svm) #Cálculo de acertos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instância, treino, teste e cálculo de acertos usando Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não executar, não funciona pois é um exemplo de como deve ser aplicada a função e não a aplicação em si.\n",
    "clf_rf = RandomForestClassifier() #Cria uma instância da Random Forest\n",
    "clf_rf.fit(X_train, Y_train) #Treinamento da Random Forest\n",
    "pred_rf = clf_rf.predict(X_test) #Teste da RF\n",
    "acc_rf = acurracy_score(Y_test, pred_rf) #Cálculo de acertos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do Componentes Principais (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Técnica que usa princípios da álgebra linear para transformar variáveis, possivelmente correlacionadas, em um número menor de variáveis chamadas de componentes principais.\n",
    "* Busca reduzir o número de dimensões em um set de dados. Projetando os dados em um novo plano.\n",
    "* Com essa nova projeção, os dados originais podem ser interpretado usando menos dimensões.\n",
    "* Com dados reduzidos pode-se observar com mais clareza tendências, padrões e outliers.\n",
    "* Fornece apenas clareza a padrões que já estão lá.\n",
    "* Objetivo: Diminuir a quatidade de atributos mantendo a acurácia.\n",
    "* Como funciona?\n",
    "    * Realiza o cálculo da média da posição dos pontos\n",
    "    * Com a média calculada, calcula a matriz de covariância.\n",
    "    * Após a matriz calculada, sendo ela real e simétrica, pode-se encontrar n autovetores (vetor que ao ser multiplicado por uma matriz retorna um valor e ao ser multiplicado por um autovalor retorna o mesmo valor da multiplicação pela matriz)\n",
    "    * Com esses autovetores forma-se uma nova matriz, ordenada pelo autovalores de cada vetor (do maior para o menor). A diagonal principal é formada pelos autovalores de cada autovetor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para calculo das componentes principais em Python usando Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não executar, não funciona pois é um exemplo de como deve ser aplicada a função e não a aplicação em si.\n",
    "from sklearn.decomposition import PCA #Biblioteca que implementa a análise de componentes principais\n",
    "\n",
    "components = [2, 4, 8, 10, 12] #Quantidade de componentes desejadas, esses valores devem ser menores que a quantidade de atributos.\n",
    "#Aqui extraímos 6 atributos de cada componente no sistema de cores RGB, totalizando 18 atributos.\n",
    "\n",
    "#Função que calcula PCA\n",
    "def pca(X_train, X_test, Y_train, n_comp):\n",
    "    \n",
    "    pca = PCA(n_components=ncomp)\n",
    "    pca.fit(X_train, Y_train)\n",
    "    transform = pca.transform(X_test)\n",
    "    return transform\n",
    "\n",
    "results = np.zeros(5) #Variável que armazena os resultados para cada componente\n",
    "\n",
    "#Para cada quantidade de componentes o PCA é executado nos conjuntos de treino e teste. Após isso os dados são classificados.\n",
    "for id_comp, comp in enumerate(components):\n",
    "    X_train_pca = pca(X_train, X_test, Y_train, comp)\n",
    "    X_test_pca = pca(X_train, X_test, Y_train, comp)\n",
    "    c_rf = RandomForestClassifier()\n",
    "    c_rf.fit(X_train_pca, Y_train)\n",
    "    pred = c_rf.predict(X_test_pca)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    results[id_comp] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código-Fonte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread_collection, imsave\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from scipy.stats import randint as sp_randint\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"databases-master/database2_raw_data\"\n",
    "\n",
    "comcapacete = glob(path+'comcapacete/*.png')\n",
    "semcapacete = glob(path+'semcapacete/*.png')\n",
    "\n",
    "images_cc = imread_collection(comcapacete)\n",
    "images_sc = imread_collection(semcapacete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(im):\n",
    "    '''Recebe uma imagem calcula limiar de otsu e fazer o \n",
    "       recorte obdecendo a região resultante desse limiar'''\n",
    "    \n",
    "    grey_image = rgb2grey(im)\n",
    "    otsu = threshold_otsu(grey_image)\n",
    "    im_ = grey_image < otsu \n",
    "    \n",
    "    n_branco = np.sum(im_ == 1)\n",
    "    n_preto = np.sum(im_ == 0)\n",
    "    if n_branco > n_preto:\n",
    "        im_ = 1-im_\n",
    "    \n",
    "    label_img = label(im_, connectivity = grey_image.ndim)#detecta regioes nao conectadas\n",
    "    props = regionprops(label_img)#calcula propriedade importantes de cada regiao encontrada (ex. area)\n",
    "\n",
    "    #Convert todas as regioes que possuem um valor de area menor que a maior area em background da imagem \n",
    "    area = np.asarray([props[i].area for i in range(len(props))])#area de cada regiao encontrada\n",
    "    max_index = np.argmax(area)#index da maior regiao\n",
    "    for i in range(len(props)):\n",
    "        if(props[i].area < props[max_index].area):\n",
    "            label_img[np.where(label_img == i+1)] = 0#regiao menor que a maior eh marcada como background\n",
    " \n",
    "    #----------------recorte da regiao de interesse----------------#\n",
    "    # Obtendo os limites verticais das imagens segmentadas \n",
    "    ymin = np.min(np.where(label_img != 0)[1])\n",
    "    ymax = np.max(np.where(label_img != 0)[1])\n",
    "    imagem_cortada = imagem[:,ymin:ymax,:]     \n",
    "    return imagem_cortada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo para segmentação das imagens:  0.0003325939178466797\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for id_im,imagem in enumerate(images_cc):\n",
    "    im_name = images_cc.files[id_im].split('/')[-1]\n",
    "    imagem_segmentada = segmentation(imagem)\n",
    "    imsave(path+'segmentacao/comcapacete/'+im_name,imagem_segmentada)\n",
    "\n",
    "    print(im_name)\n",
    "for id_im,imagem in enumerate(images_sc):\n",
    "    im_name = images_sc.files[id_im].split('/')[-1]\n",
    "    imagem_segmentada = segmentation(imagem)\n",
    "    imsave(path+'segmentacao/semcapacete/'+im_name,imagem_segmentada)\n",
    "    print(im_name)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"Tempo para segmentação das imagens: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((np.zeros(len(comcapacete)),np.ones(len(semcapacete))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de características usando GLCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_segmentada = \"databases-master/database2_raw_data/segmentacao/\"\n",
    "comcapacete = glob(path_segmentada+'comcapacete/*.png')\n",
    "semcapacete = glob(path_segmentada+'semcapacete/*.png')\n",
    "images = imread_collection(comcapacete+semcapacete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-26845518d903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreycoprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'correlation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreycoprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ASM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid_ch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_ch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "d = 15\n",
    "\n",
    "features = np.zeros((len(labels),18)) #6 features x 3 color channels\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for id_im,imagem in enumerate(images):\n",
    "    for id_ch in range(3):\n",
    "        matrix0 = greycomatrix(imagem[:,:,id_ch], [d], [0],normed=True)\n",
    "        matrix1 = greycomatrix(imagem[:,:,id_ch], [d], [np.pi/4],normed=True)\n",
    "        matrix2 = greycomatrix(imagem[:,:,id_ch], [d], [np.pi/2],normed=True)\n",
    "        matrix3 = greycomatrix(imagem[:,:,id_ch], [d], [3*np.pi/4],normed=True)\n",
    "        matrix = (matrix0+matrix1+matrix2+matrix3)/4 \n",
    "        props = np.zeros((6))\n",
    "        props[0] = greycoprops(matrix,'contrast')\n",
    "        props[1] = greycoprops(matrix,'dissimilarity')\n",
    "        props[2] = greycoprops(matrix,'homogeneity')\n",
    "        props[3] = greycoprops(matrix,'energy')\n",
    "        props[4] = greycoprops(matrix,'correlation')\n",
    "        props[5] = greycoprops(matrix,'ASM')\n",
    "        features[id_im,id_ch*6:(id_ch+1)*6] = props\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hidel/anaconda3/envs/AcucenaICV/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "train = 0.5\n",
    "test = 1-train\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer, confusion_matrix,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Random Forest Parameter Estimation\n",
    "def rf_parameter_estimation(xEst, yEst):\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=20)    \n",
    "    # specify parameters and distributions to sample from\n",
    "    hyperparameters = {\"n_estimators\": range(10,1000,50),\n",
    "                  \"max_depth\": range(1,100),\n",
    "                  \"max_features\": sp_randint(1, xEst.shape[1]),\n",
    "                  \"min_samples_split\": sp_randint(1, xEst.shape[1]),\n",
    "                  \"min_samples_leaf\": sp_randint(1, xEst.shape[1]),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    \n",
    "    # run randomized search\n",
    "    n_iter_search = 20\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=hyperparameters,\n",
    "                                       n_iter=n_iter_search,scoring=make_scorer(accuracy_score))\n",
    "    \n",
    "    \n",
    "    random_search.fit(xEst, yEst)\n",
    "    report(random_search.cv_results_)\n",
    "    return random_search.best_params_\n",
    "\n",
    "#SVM Parameter Estimation\n",
    "def svm_parameter_estimation(xEst, yEst):\n",
    "    \n",
    "    hyperparameters = {'gamma': [1e-1, 1e-2,1e-3, 1e-4],'C': [1, 10, 100, 1000]}\n",
    "    \n",
    "    clf = SVC(kernel='rbf')\n",
    "    # Run randomized search\n",
    "    n_iter_search = 8\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=hyperparameters,\n",
    "                                       n_iter=n_iter_search,scoring=make_scorer(accuracy_score))    \n",
    "    random_search.fit(xEst, yEst)\n",
    "\n",
    "    report(random_search.cv_results_)\n",
    "    return random_search.best_params_\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-35300f455843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_parameter_estimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mc_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4471a72e2c2a>\u001b[0m in \u001b[0;36mrf_parameter_estimation\u001b[0;34m(xEst, yEst)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxEst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myEst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AcucenaICV/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AcucenaICV/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \"\"\"\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AcucenaICV/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    460\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 462\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "parameters = rf_parameter_estimation(X_train, y_train)\n",
    "c_rf = RandomForestClassifier(**parameters)\n",
    "c_rf.fit(X_train,y_train)\n",
    "pred = c_rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, pred)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "print('Random Forest Accuracy: ',acc_rf)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = svm_parameter_estimation(X_train, y_train)\n",
    "c_svm = SVC(**parameters)\n",
    "c_svm.fit(X_train,y_train)\n",
    "pred = c_svm.predict(X_test)\n",
    "acc_svm = accuracy_score(y_test, pred)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "parameters\n",
    "\n",
    "print('Support Vector Machine Accuracy: ',acc_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tutorial Visão Computacional.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
